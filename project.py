# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14rG5XNhK1-NmDprSbkOrBVEl1ZMi7xTM

Load the images from the VISCHEMA dataset
"""

content = '/content/drive/MyDrive/Colab Notebooks/Project/'

import torch
from torchvision.datasets.utils import extract_archive
import numpy as np
from PIL import Image
import csv
import os
from torchvision import transforms
import matplotlib.pyplot as plt
import time


extract_archive(content+'VISCHEMA.zip','/content')
extract_archive(content+'VISCHEMA_VMS.zip','/content')

"""Flatten the directories, load them into memory and convert them to pytorch tensors."""

def find_files(directory):
    for d, dirs, files in os.walk(directory):
        for f in files:
            yield os.path.join(d, f)

image_gen = find_files('/content/VISCHEMA SUN folders')
#image_gen = find_files(f'{content}TEST VISCHEMA')

PRETRAINED = True
SEGMENTATION = False

if PRETRAINED:  
  if SEGMENTATION: X = Y = 100
  else: X = Y = 224
else:
  X = Y = 160

if SEGMENTATION: vms_x = vms_y = 100
else: vms_x = vms_y = 20

# Resize to 140x140 pixels. (Might need to be 224x224 when using a pretrained model https://pytorch.org/vision/stable/models.html)
if PRETRAINED:
  # Normalise data to [0,1] and normalise mean and std to be used for transfer learning
  if SEGMENTATION:
    transform = transforms.Compose([
      transforms.Resize((X,Y)),
      transforms.ToTensor(),
      transforms.Lambda(lambda x: x/255)
    ])
  else:
    transform = transforms.Compose([
      transforms.Resize((X,Y)),
      transforms.ToTensor(),
      transforms.Lambda(lambda x: x/255),
      transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225]),
    ])
else:
  transform = transforms.Resize((X,Y))

images = [i for i in image_gen]
# Use custom sorting key to align images with vms, irregardless of directory structure
images.sort(key=lambda x: x.split('/')[-1])

classes = [c.split('/')[4] for c in images]
data = [np.asarray(transform(Image.open(f))) for f in images]

data = np.array(data)
data = torch.FloatTensor(data)

vms_gen = find_files('/content/VISCHEMA VMS')
#vms_gen = find_files(f'{content}TEST VISCHEMA VMS')

transform = transforms.Resize((vms_x,vms_y))

vms = [v for v in vms_gen]
# Use custom sorting key to align images with vms, irregardless of directory structure
vms.sort(key=lambda x: x.split('/')[-1])

# Use only the coloured vms maps.
# outputs = [(np.asarray(transform(Image.open(f)))/255).flatten() for f in vms if '_S' not in f]

# Use the non-coloured vms maps.
outputs = [((np.asarray(transform(Image.open(f))))/255).flatten() for f in vms if '_S' in f]

outputs = np.array(outputs)
outputs = torch.FloatTensor(outputs)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(data.shape,outputs.shape, len(classes))

"""Initialise a custom Dataset to store the VISCHEMA images, vms maps and classes."""

from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split

class VISCHEMA(Dataset):
  def __init__(self,data,vms,classes):
    self.data = data
    self.vms = vms
    self.classes = classes
  def __len__(self):
    return self.data.shape[0]
  def __getitem__(self,id):
    image = (self.data[id,:,:]).unsqueeze(0)
    if self.vms == None: return image
    else:
      vms = self.vms[id]
    if self.classes == None: return image, vms
    else:
      return image, vms, self.classes[id]

# Use 5% of the dataset (40 images for VISCHEMA) for testing. 
TEST_SIZE = 0.05
BATCH_SIZE = 4

full_dataset = VISCHEMA(data,outputs,classes)

# Perform a stratified train - test split on our dataset, such that both sets should have a similar amount of samples from each class.

train_indices, test_indices, _, _ = train_test_split(range(data.shape[0]),classes,stratify=classes,test_size=TEST_SIZE)
train_split = Subset(full_dataset,train_indices)
test_split = Subset(full_dataset,test_indices)

# Initialise DataLoaders for train and test batches.

dataloaders = {'train':DataLoader(train_split, batch_size=BATCH_SIZE,shuffle=True,num_workers=1),
               'test':DataLoader(test_split, batch_size=BATCH_SIZE,shuffle=True,num_workers=1)}

dataset_sizes = {'train': len(train_split), 'test': len(test_split)}

import random

# Crops the image and vms randomly, while keeping them consistent with one another.
def random_transform(image, vms):
  vms = transforms.functional_tensor.resize(vms[:].cpu().squeeze().reshape((vms_x, vms_y)).unsqueeze(0),(X,Y)).squeeze(0)
  rnd_width = rnd_height = random.randint(28,X) 

  rnd_x = random.randint(0,X - rnd_width)
  rnd_y = random.randint(0,Y - rnd_height)

  image = transforms.functional_tensor.crop(image, rnd_y, rnd_x, rnd_height, rnd_width)
  vms = transforms.functional_tensor.crop(vms, rnd_y, rnd_x, rnd_height, rnd_width)

  image = transforms.functional_tensor.resize(image.unsqueeze(0), X).squeeze(0)
  vms = transforms.functional_tensor.resize(vms.unsqueeze(0), vms_x).squeeze(0)

  vms = torch.flatten(vms)
  return image,vms

def show_images(num, model=None, r_transform=False):
  if model != None:
    was_training = model.training
    model.eval()

  
  image_count = 0
  fig = plt.figure(time.time(), figsize=(15,15))
  cols = rows = int(np.sqrt(num))
  transform = transforms.Resize((X,Y))
  invNormalise = transforms.Compose([transforms.Normalize(mean = [0., 0., 0.],
                                                          std = [1/0.229, 1/0.224, 1/0.225]),
                                     transforms.Normalize(mean = [-0.485, -0.456, -0.406],
                                                          std = [1., 1., 1.])])
  with torch.no_grad():
    for _, (inputs, vms, labels) in enumerate(dataloaders['test']):
      
      if model != None:
        inputs = inputs[:,0,:,:,:].to(device)
        vms = vms.to(device)
        

        outputs = model(inputs)
        if r_transform:
          for i in range(inputs.shape[0]):
            inputs[i,:,:,:],outputs[i,:] = random_transform(inputs[i],outputs[i])
        
      for i in range(num):
        fig.add_subplot(rows, cols, i+1)
        if model != None:
          plt.title(f'Loss: {criterion(outputs[i], vms[i])} ',color='black')
        plt.axis("off")
        # Display image
        plt.imshow(np.asarray(((invNormalise(inputs[i,:].cpu()))).squeeze().permute(1,2,0))*255)
        # We need to convert vms array to image so it can be resized.
        if model:
          vms_image = Image.fromarray(np.asarray(outputs[i,:].cpu().squeeze()*255).reshape((vms_x,vms_y)).astype(np.uint8))
        else:
          vms_image = Image.fromarray(np.asarray(vms[i,:].cpu().squeeze()*255).reshape((vms_x,vms_y)).astype(np.uint8))
        # Display vms
        plt.imshow(transform(vms_image),alpha=1)
      plt.show()
      if model:
        model.train(mode=was_training)
      break

import torchvision.transforms.functional as F
from torchvision.utils import draw_segmentation_masks

def show_imgs(num, model):
  if model != None:
    was_training = model.training
    model.eval()

  with torch.no_grad():
    for _, (inputs, vms, labels) in enumerate(dataloaders['test']):

      if model != None:
        inputs = inputs[:,0,:,:,:].to(device)
        vms = vms.to(device)
        outputs = model(inputs)
        outputs = model(inputs)['out']
        outputs = (outputs.argmax(1) == 1)
        imgs_with_masks = [
          draw_segmentation_masks((img.cpu()*255).squeeze().to(torch.uint8), masks=mask, alpha=0.7)
          for img, mask in zip(inputs, outputs)
        ]
        show(imgs_with_masks)
      if model:
        model.train(mode=was_training)
      break
  

def show(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = F.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

"""Visualise some sample images with the VMS overlayed."""

import matplotlib.pyplot as plt

images,vms,labels = next(iter(dataloaders['test']))

show_images(4)

"""Set up model to use for transfer learning"""

import torchvision.models as models
from torch import optim

if SEGMENTATION:
  model = models.segmentation.fcn_resnet101(pretrained=True)
  model.classifier = torch.nn.Sequential(
      torch.nn.Conv2d(2048,2048,kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False),
      torch.nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
      torch.nn.ReLU(),
      torch.nn.Dropout(p=0.1, inplace=False),
      torch.nn.Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1)),
      torch.nn.Conv2d(1024,512,kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=False),
      torch.nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
      torch.nn.ReLU(),
      torch.nn.Dropout(p=0.1, inplace=False),
      torch.nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1)))
  
  for param in model.parameters():
    param.requires_grad = True

  criterion = torch.nn.CrossEntropyLoss()
  params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in (model.classifier.parameters() or model.classifier.aux_parameters()), model.named_parameters()))))
  base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in (model.classifier.parameters() or model.classifier.aux_parameters()), model.named_parameters()))))
  vms_threshold = 0.5

else:
  model = models.resnet101(pretrained=True)
  fc_ftrs = model.fc.in_features
  model.fc = torch.nn.Sequential(
      torch.nn.Linear(fc_ftrs,2048),
      torch.nn.ReLU(),
      torch.nn.Linear(2048,1024),
      torch.nn.ReLU(),
      torch.nn.Linear(1024,512),
      torch.nn.ReLU(),
      torch.nn.Linear(512,vms_x*vms_y),
      torch.nn.Sigmoid()
  )
  params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] in model.fc.parameters(), model.named_parameters()))))
  base_params = list(map(lambda x: x[1],list(filter(lambda kv: kv[0] not in model.fc.parameters(), model.named_parameters()))))
  criterion = torch.nn.MSELoss()

model = model.to(device)

optimiser = optim.SGD([{"params" : base_params, "lr": 0.00001, "momentum":0.9},{"params" : params, "lr": 0.001, "momentum":1}])

exp_lr_scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=25, gamma=0.9)

import time
import copy
from torchvision.utils import make_grid

RANDOM_TRANSFORM = True

def train_model(model, criterion, optimiser, scheduler, epochs=25):
  start = time.time()
  best_weights = copy.deepcopy(model.state_dict())
  best_loss = None
  loss_list = [[],[]]
  epoch_list = [[],[]]
  transform = transforms.Lambda(lambda x: x*255)
  for epoch in range(epochs+1):
    print(f'Epoch {epoch}/{epochs}\n---------------\n')

    for phase in ['train', 'test']:
      if phase == 'train':
        model.train()
      else: model.eval()

      tot_loss = 0.0

      for inputs, vms, labels in dataloaders[phase]:
        inputs = inputs[:,0,:,:,:].to(device)
        
        if SEGMENTATION: vms = (vms.to(device) > vms_threshold).float()
        else: vms = vms.to(device)
        
        if RANDOM_TRANSFORM:
          for i in range(inputs.shape[0]):
            inputs[i,:,:,:],vms[i,:] = random_transform(inputs[i],vms[i])

        optimiser.zero_grad()

        with torch.set_grad_enabled(phase == 'train'):
          if SEGMENTATION:
            outputs = model(inputs)['out']
            outputs = (outputs.argmax(1)).to(float)
            loss = criterion(outputs, vms)
          
          else:
            outputs = transform(model(inputs))
            loss = criterion(outputs, transform(vms))
          
          if phase == 'train':
            loss.backward()
            optimiser.step()

        tot_loss += loss.item() * inputs.size(0)

      if phase == 'train':
        scheduler.step()
      epoch_loss = tot_loss / dataset_sizes[phase]
      if phase == 'train':
        loss_list[0].append(epoch_loss)
        epoch_list[0].append(epoch)
      else:
        loss_list[1].append(epoch_loss)
        epoch_list[1].append(epoch)

      print(f'{phase} loss: {epoch_loss:.4f}\n')
      if phase == 'test' and (best_loss == None or best_loss > epoch_loss):
        best_loss = epoch_loss
        best_weights = copy.deepcopy(model.state_dict())
  model.load_state_dict(best_weights)

  plt.plot(epoch_list[0],loss_list[0],label="Train")
  plt.plot(epoch_list[1],loss_list[1],label="Test")
  plt.legend()
  plt.xlabel("Epoch")
  plt.ylabel("MSE Loss")
  plt.savefig(f'{content}input-2048-1024-512-output-(lr=0.001&0.00001)-(stepsize=25&gamma=0.9)-randomTransform.pdf')
  plt.show()
  
  return model

model = train_model(model, criterion, optimiser, exp_lr_scheduler, epochs=500)

show_images(4,model)